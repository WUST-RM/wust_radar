import cv2
import yaml
import torch
import numpy as np
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
from PIL import Image
import os

# â€”â€” 0. å‚æ•°æ–‡ä»¶è·¯å¾„ & è§†é¢‘è·¯å¾„ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
yaml_path = "/home/hy/wust_radar/src/wust_binocular/config/camera_info.yaml"
input_path = "/home/hy/data/video_save/å›½ç§‘å¤§1.avi"
left_path = "left.mp4"
right_path = "right.mp4"
stereo_path = "stereo.mp4"

# â€”â€” 1. åŠ è½½ YAML ä¸­çš„ç›¸æœºå‚æ•° â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
with open(yaml_path, "r") as f:
    cam = yaml.safe_load(f)

K_data = cam["camera_matrix"]["data"]
K = np.array(K_data, dtype=np.float32).reshape(3, 3)
dist = np.array(cam["distortion_coefficients"]["data"], dtype=np.float32).reshape(1, -1)

# â€”â€” 2. æ‰‹åŠ¨è®¾ç½®åŒç›®åŸºçº¿ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
baseline = 0.06  # 6 cm

# â€”â€” 3. åˆå§‹åŒ– MiDaS æ·±åº¦æ¨¡å‹ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

midas = (
    torch.hub.load("intel-isl/MiDaS", "MiDaS_small", pretrained=True, revision="master")
    .to(device)
    .eval()
)

transform = Compose(
    [
        Resize((384, 384)),
        ToTensor(),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ]
)

# â€”â€” 4. è§†é¢‘è¯»å– & å†™å…¥é…ç½® â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
cap = cv2.VideoCapture(input_path)
if not cap.isOpened():
    raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {input_path}")

frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
if frame_count == 0:
    print("âš ï¸ è­¦å‘Šï¼šæ— æ³•è¯»å–è§†é¢‘å¸§æ•°ï¼Œframe_count == 0ï¼Œè¿›åº¦æ‰“å°å°†ä¸å‡†ç¡®ã€‚")

fps = cap.get(cv2.CAP_PROP_FPS)
w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
writer_left = cv2.VideoWriter(left_path, fourcc, fps, (w, h))
writer_right = cv2.VideoWriter(right_path, fourcc, fps, (w, h))
writer_stereo = cv2.VideoWriter(stereo_path, fourcc, fps, (w * 2, h))

map1, map2 = cv2.initUndistortRectifyMap(K, dist, np.eye(3), K, (w, h), cv2.CV_32FC1)


# â€”â€” 5. åˆæˆå³ç›®å›¾åƒ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def synthesize_right(frame, depth, K, baseline):
    j, i = np.indices(depth.shape, dtype=np.float32)
    fx, fy = K[0, 0], K[1, 1]
    cx, cy = K[0, 2], K[1, 2]
    Z = np.maximum(depth, 1e-3)
    X = (i - cx) * Z / fx
    Y = (j - cy) * Z / fy
    Xr = X - baseline
    ur = (Xr * fx / Z + cx).astype(np.float32)
    vr = j
    return cv2.remap(
        frame,
        ur,
        vr,
        interpolation=cv2.INTER_LINEAR,
        borderMode=cv2.BORDER_CONSTANT,
        borderValue=0,
    )


# â€”â€” 6. ä¸»å¾ªç¯ï¼šè¯»å– â†’ å»ç•¸ â†’ æ·±åº¦ä¼°è®¡ â†’ åˆæˆå³ç›® â€”â€”â€”â€”â€”â€”â€”â€”â€”
with torch.no_grad():
    idx = 0
    while True:
        ret, raw = cap.read()
        if not ret:
            break
        idx += 1

        # è¿›åº¦æ‰“å°ï¼ˆæ¯ 30 å¸§ä¸€æ¬¡ï¼‰
        if idx % 30 == 0:
            print(f"[{idx}/{frame_count}] Processing...")

        # å·¦ç›®å›¾åƒ
        left = cv2.remap(raw, map1, map2, interpolation=cv2.INTER_LINEAR)

        # æ·±åº¦ä¼°è®¡è¾“å…¥
        img_rgb = cv2.cvtColor(left, cv2.COLOR_BGR2RGB)
        img_pil = Image.fromarray(img_rgb)
        input_tensor = transform(img_pil).unsqueeze(0).to(device)  # [1, 3, 384, 384]

        pred = midas(input_tensor)
        if pred.ndim == 3:
            pred = pred.unsqueeze(1)  # [1, 1, H, W]

        # ä¸Šé‡‡æ ·ä¸ºåŸå›¾å¤§å°
        depth = (
            torch.nn.functional.interpolate(
                pred, size=(h, w), mode="bicubic", align_corners=False
            )
            .squeeze()
            .cpu()
            .numpy()
        )

        # æ·±åº¦å½’ä¸€åŒ–ï¼ˆè½¬æ¢ä¸º 0.1m ~ 5.1m èŒƒå›´ï¼‰
        depth = (depth - np.min(depth)) / (np.max(depth) - np.min(depth) + 1e-6)
        depth = depth * 25.0 + 0.1

        # åˆæˆå³ç›®å›¾åƒ
        right = synthesize_right(left, depth, K, baseline)

        # å†™å…¥è§†é¢‘
        stereo = np.hstack([left, right])
        writer_left.write(left)
        writer_right.write(right)
        writer_stereo.write(stereo)

        # è‹¥æ—  GUI æ˜¾ç¤ºç¯å¢ƒï¼Œå»ºè®®æ³¨é‡Šä»¥ä¸‹ä¸¤è¡Œä»¥é¿å… Qt æŠ¥é”™
        # cv2.imshow("Stereo Preview", cv2.resize(stereo, (w, h)))
        # if cv2.waitKey(1) == 27: break

# â€”â€” 7. é‡Šæ”¾èµ„æº â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
cap.release()
writer_left.release()
writer_right.release()
writer_stereo.release()
cv2.destroyAllWindows()

print("âœ… å¤„ç†å®Œæˆï¼è¾“å‡ºç»“æœï¼š")
print(f"  ğŸ‘ å·¦ç›®è§†é¢‘ï¼š{left_path}")
print(f"  ğŸ‘ å³ç›®è§†é¢‘ï¼š{right_path}")
print(f"  ğŸ ç«‹ä½“è§†é¢‘ï¼š{stereo_path}")
